

spark.jars /opt/hudi/hudi-spark-bundle.jar,/opt/hadoop/share/hadoop/hdfs/lib/hadoop-aws-3.2.0.jar,/usr/local/hive/lib/hive-exec.jar
spark.serializer org.apache.spark.serializer.KryoSerializer

CREATE EXTERNAL TABLE hudi_table (
  id STRING,
  name STRING,
  ts TIMESTAMP
)
STORED AS PARQUET
LOCATION 'hdfs://localhost:9000/user/hudi_table';


// Hudi library to work with Hudi tables
libraryDependencies += "org.apache.hudi" %% "hudi-spark-bundle" % "0.9.0"

// Spark core and SQL libraries to work with Spark
libraryDependencies += "org.apache.spark" %% "spark-core" % "3.2.3"
libraryDependencies += "org.apache.spark" %% "spark-sql" % "3.2.3"



import org.apache.spark.sql.SparkSession
import org.apache.hudi.DataSourceWriteOptions._
import org.apache.hudi.config.HoodieWriteConfig._
import org.apache.spark.sql.SaveMode

val spark = SparkSession.builder()
  .appName("Hudi Test")
  .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
  .getOrCreate()

val data = Seq((1, "Alice", "2023-09-17T12:00:00Z"),
               (2, "Bob", "2023-09-17T12:05:00Z"))

val df = spark.createDataFrame(data).toDF("id", "name", "ts")

df.write.format("hudi")
  .option(TABLE_NAME, "hudi_table")
  .option(RECORDKEY_FIELD_OPT_KEY, "id")
  .option(PRECOMBINE_FIELD_OPT_KEY, "ts")
  .mode(SaveMode.Append)
  .save("hdfs://localhost:9000/user/hudi_table")




spark-shell --conf "spark.executor.extraJavaOptions=-Dlog4j.configuration=file:/path/to/log4j.properties"





from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Hudi Test") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.jars", "/opt/hudi/hudi-spark-bundle.jar") \
    .getOrCreate()


data = [(1, "Alice", "2023-09-17T12:00:00Z"), 
        (2, "Bob", "2023-09-17T12:05:00Z")]

df = spark.createDataFrame(data, ["id", "name", "ts"])


df.write.format("hudi") \
    .option("hoodie.table.name", "hudi_table") \
    .option("hoodie.datasource.write.recordkey.field", "id") \
    .option("hoodie.datasource.write.precombine.field", "ts") \
    .mode("append") \
    .save("hdfs://localhost:9000/user/hudi_table")


USE hudi_test;
SELECT * FROM hudi_table;






import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.hudi.DataSourceWriteOptions // Import only DataSourceWriteOptions

object HudiTest {
  def main(args: Array[String]): Unit = {
    
    // Create Spark session
    val spark = SparkSession.builder()
      .appName("Hudi Test")
      .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .config("spark.jars", "/opt/hudi/hudi-spark-bundle.jar")  // Specify Hudi jar
      .getOrCreate()

    // Sample data
    val data = Seq(
      (1, "Alice", "2023-09-17T12:00:00Z"),
      (2, "Bob", "2023-09-17T12:05:00Z")
    )

    // Create DataFrame
    val df = spark.createDataFrame(data).toDF("id", "name", "ts")

    // Write Data to Hudi table with qualified references to avoid ambiguity
    df.write.format("hudi")
      .option(DataSourceWriteOptions.TABLE_NAME, "hudi_table_test") // Qualified reference
      .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY, "id") // Qualified reference
      .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY, "ts") // Qualified reference
      .mode(SaveMode.Append)
      .save("hdfs://localhost:9000/user/hudi_table_test")

    // Stop the Spark session
    spark.stop()
  }
}



import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.hudi.DataSourceWriteOptions

object HudiTest {
  def main(args: Array[String]): Unit = {

    // Create Spark session
    val spark = SparkSession.builder()
      .appName("Hudi Test")
      .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .config("spark.jars", "/opt/hudi/hudi-spark-bundle.jar")  // Specify Hudi jar
      .getOrCreate()

    // Sample data
    val data = Seq(
      (1, "Alice", "2023-09-17T12:00:00Z"),
      (2, "Bob", "2023-09-17T12:05:00Z")
    )

    // Create DataFrame
    val df = spark.createDataFrame(data).toDF("id", "name", "ts")

    // Write Data to Hudi table using string keys instead of ConfigProperty
    df.write.format("hudi")
      .option("hoodie.table.name", "hudi_table_test")  // Use string key instead of ConfigProperty
      .option("hoodie.datasource.write.recordkey.field", "id")  // Use string key
      .option("hoodie.datasource.write.precombine.field", "ts")  // Use string key
      .mode(SaveMode.Append)
      .save("hdfs://localhost:9000/user/hudi_table_test")

    // Stop the Spark session
    spark.stop()
  }
}



